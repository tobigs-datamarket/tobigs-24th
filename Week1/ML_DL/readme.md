# ML_DL

## 인공지능(AI)

인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현시키는 컴퓨터 과학 기술

가장 포괄적인 개념으로 인간의 지능을 모방하는 모든 기술

---

## 머신러닝(ML)

AI의 하위 개념으로 데이터를 통해 자동으로 학습하는 능력

[목적]

데이터를 통해 학습하며 이 학습의 핵심은 가중치 행렬(Weight Matrix)을 최적화하는 과정

- 가중치행렬이란?

  모델이 입력 데이터를 바탕으로 예측을 수행시 각 입력 특성에 부여하는 중요도를 나타내는 값들의 집합, 주로 행렬 형태로 표현

(과정 반복)

모델은 예측 결과와 실제 값 사이의 오차(손실)를 계산

$\rightarrow$ 이 오차를 줄이기 위해 손실 함수의 기울기를 계산

$\rightarrow$ 이 기울기를 바탕으로 가중치 행렬 값을 조금씩 갱신

[파이프라인 개요]

데이터 준비 & 이해 > EDA 및 전처리 > 피처 엔지니어링 > 모델링 > 하이퍼 파라미터 튜닝 > 평가 및 피드백

1. 데이터 준비 및 이해 : 도메인 이해, 기본 통계량 확인, 메타데이터 검토
2. EDA 및 전처리
3. 피처 엔지니어링 : 머신 러닝 모델 성능 향상을 위해 원본 데이터를 분석하고 가공하여 유용한 특징(피처)을 만드는 과정
4. 모델링 : 전처리된 데이터를 기반으로 알고리즘을 선택,학습시켜 에측 함수를 만드는 과정

   주어진 문제에 최적화된 모델 파라미터 찾기 $\rightarrow$ 모델링 역시 가중치행렬을 찾기 위한 과정

   분류(Classification)|회귀(Regression)|차원 축소(Dimensionality Reduction)|클러스터링(Clustering)
   -|-|-|-
   로지스틱 회귀 <br> 서포트 벡터 머신(SVM) <br> 랜덤 포레스트 <br> 결정 트리 <br> KNN|선형 회귀 <br> 리지,라쏘 회귀 <br> 서포트 벡터 회귀(SVR)|주성분 분석(PCA) <br> t-SNE/UMAP <br> 오토인코더(AutoEncoder)|K-Means <br> DBSCAN <br> 계층적 군집화

5. 하이퍼 파라미터 튜닝

   하이퍼파라미터 : 모델 구조나 학습 알고리즘에 외부에서 설정하는 값

   -|파라미터|하이퍼 파라미터
   -|-|-
   정의|자동으로 최적화|수동으로 설정
   설정 시점|학습 중|학습 전
   조정 방법|경사하강법 등 최적화 알고리즘으로 자동 업데이터|그리드서치, 랜덤서치, 베이지안 최적화 등 실험적 탐색
   영향 범위|모델의 예측 함수 형태에 직접 영향|학습 속도, 성능, 일반화 능력에 간접 영향

   튜닝의 필요성 : 잘못 설정 시 과소적합/과적합 초래, 최적 하이퍼파라미터로 모델 성능과 일반화 능력 극대화

   - 그리드서치
  
     사용자가 지정한 값들의 격자(grid)에 따라 가능한 모든 조합을 일괄 탐색하여 최적의 설정을 찾아내는 방식

     장점 : 설정된 공간 내에서 최적 조합 보장

     단점 : 조합 수가 증가하면 연산 수가 기하급수적으로 증가

   - 랜덤서치
  
     전체 가능한 파라미터 공간에서 무작위 일부 조합을 샘플링해 실험하는 방법

     장점 : 소수의 시도만으로도 준수한 성능 확보 가능 > 시간 단축

     단점 : 무작위 과정에서 최적의 하이퍼파라미터 놓칠 수 있음

   - 베이지안 최적화
  
     사전 정보를 바탕으로 최적 하이퍼파라미터 값을 확률적으로 추정하여 탐색하는 기법

     그리드서치나 랜덤서치보다 더 빠르고 효율적

     - 확률 모형(Surrogate Model) : 과거 실험 데이터를 바탕으로 아직 실험하지 않은 지점의 성능을 확률적으로 예측하는 모델, 가우시안 프로세스(Gaussian Process, GP) 사용
    
     - 획득 함수(Acquisition Function) : 확률 모델의 예측을 바탕으로 다음으로 실험할 가장 유망한 하이퍼파라미터 조합을 결정하는 함수, Expected Improvement(EI) 사용
    
      관측 > 모델링 > 결정 > 결정된 지점에서 다시 실험을 수행

     장점 : 적은 시도로 효율적인 최적화 가능

     단점 : 최적화할 하이퍼파라미터 수가 매우 많아지면 효율성 떨어지고 계산 비용 증가

6. 평가 및 피드백

### 과대적합, 과소적합

- **과소적합(Underfitting)**

  너무 단순한 모델 > 중요한 패턴 학습 불가 > 학습,검정 에러 모두 높음 > 모델링 잘못됨

- **과대적합(Overfitting)**

  너무 복잡한 모델 > 노이즈까지 학습 > 학습 에러 낮고 검증 에러 급증 > 일반화 필요

### 편향, 분산

- **편향(Bias)** : 모델 예측 평균과 실제값 차이
- **분산(Variance)** : 학습 시마다 예측값 변동성

### 손실함수 & 평가지표

**손실함수**

모델의 예측값과 실제값 간의 차이를 수치화해주는 함수

[역할]

모델 평가 : 손실 함수 값이 작을수록 모델의 예측이 실제 값에 가깝다는 것을 의미

학습 목표 설정 : 모델의 목표는 손실 함수 값 최소화하기, 모델은 학습 과정에서 가중치를 조정하며 손실 함수 값을 줄여감

회귀용 손실함수 : MSE, MAE

분류용 손실함수 : Cross-Entropy, Hinge Loss 등

$\rightarrow$ 손실함수를 최소화하기 위해 적절한 optimizer 선택 필요

- **MSE(Mean Squared Error)**

  실제값과 예측값의 차이를 제곱한 뒤 전체 샘플 수로 나눈 평균

  $MSE = \frac{1}{n}\displaystyle\sum_{i=1}^n(y_i - \hat y_i)^2$

  주로 **회귀(Regression)**에서 사용, 연속적인 숫자 값 예측에 적합

  예측 오차의 '크기'를 제곱하여 손실 측정

  항상 0 이상의 값을 가지며 0에 가까울수록 예측 정확 의미

  미분 가능하며 경사 하강법 기반 최적화에 용이함

  오차를 제곱하기 때문에 이상치에 민감하게 반응할 수 있음

- **Cross-Entropy**

  예측된 확률 분포가 실제(정답) 확률 분포와 얼마나 다른지를 측정

  분포 간의 '거리'를 나타냄

  $L = -\frac{1}{n}\displaystyle\sum_{i=1}^n[y_i log \hat y_i + (1-y_i)log(1-\hat y_i)]$

  **분류(Classification_** 문제, 모델의 출력이 확률 분포일 때 사용

  모델이 정답 클래스에 낮은 확률을 부여하거나 오답 클래스에 높은 확률을 부여할수록 큰 손실 반환

- **혼동행렬**

  분류 모델의 예측 결과와 실제 값을 교차표 형태로 정리한 행렬

  .|예측값 - 0(Negative)|예측값 - 1(Positive)
  -|-|-
  실제값 - 0(Negative)|TN(참음성)|FP(거짓양성)**1종 오류**
  실제값 - 1(Positive)|FN(거짓음성)**2종 오류**|TP(참양성)

  - 1종오류 : 실제 음성을 양성으로 잘못 판정
 
  - 2종오류 : 실제 양성을 음성으로 잘못 판정

**평가지표**

- 정확도(Accuracy) : 전체 예측 중 올바른 예측 비율

  $\frac{TP+TN}{TP+TN+FP+FN}$
  
- 재현율(Recall) : 실제 양성 중 양성으로 올바르게 예측한 비율
  
  $\frac{TP}{TP+FN}$
  
- 정밀도(Precision) : 양성으로 예측한 것 중 실제 양성 비율
  
  $\frac{TP}{TP+FP}$
  
- F1-Score : 정밀도와 재현율의 조화 평균
  
  $\frac{2 \times Precision \times Recall}{Precision + Recall}$

+) **ROC 커브**

  분류 임계값을 변화시키며 민감도와 1-특이도의 관계를 시각화한 곡선

  임계값을 여러 값으로 바꿔가며 모델 전반적인 분류 능력을 평가할 때, 모델의 전체적인 성능을 비교하고 싶을 때, 클래스의 불균형이 심하지 않은 이진 분류 문제에서 모델 선택 기준으로 활용할 때 사용함

---

## 딥러닝(DL)

ML의 하위 개념으로 인공신경망을 활용한 학습 방법

### ANN(Artifical Neural Network)

입력층, 은닉층, 출력층으로 구성된 모든 신경망 구조

뉴런(노드)들이 서로 연결되어 정보를 전달, 처리함

은닉층 개수, 연결 방식, 구조에 상관없이 가장 포괄적 개념

ex) 얇은 신경망, DNN, CNN, RNN, Transformer 등

### DNN(Deep Neural Network)

은닉층이 2개 이상인 ANN뉴런(노드)들이 서로 연결되어 정보를 전달, 처리함

보통 Fully Connected(FC) Layer 중심으로 구성됨

### 퍼셉트론
