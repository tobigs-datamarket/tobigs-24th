# 기초통계

---

## CLT(Central Limit Theorem)

[정의]

모집단의 분포에 상관없이 크기가 n인 표본은 반복적으로 추출하여 표본 평균을 계산할 때 **n이 충분히 크다**면 표본평균의 분포는 **정규분포(NormalDistribution)**에 가까워짐

[이점]
1. 정규분포의 보편성
2. 샘플 기반 추론 - 모집단 분포를 몰라도 표본평균 통해 추론 가능
3. 기계학습과 AI에서의 활용 - Cross-validation의 이론적 근거

[조건]
1. Independence
2. Identical Distribution
3. Sample Size : 일반적으로 n $\ge$ 30
4. Finite Variance : 무한한 분산은 데이터가 퍼져 의미 해석력이 떨어짐
5. 표본 크기와 모집단 비율 : 모집단이 유한한 경우 표본크기 <= 모집단 크기 10%

---

## Bayes' Theorem

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

- $P(A|B)$
  
  사후 확률 : B(데이터)를 확인한 후에 A(우리가 생각하는 가설)의 확률
- $P(B|A)$

  우도 : A(가설)가 맞다고 가정하였을 떄 B(데이터)가 나올 확률

- $P(A)$ : 사전 확률
  
- $P(B)$ : 정규화 상수

사전 확률을 어떠한 처리를 통해 사후 확률 계산 가능 $\rightarrow$ 점진적 업데이트 가능 = 전체 데이터를 다시 학습하지 않고도 예측력 유지 가능

사용 예시) MAP, BIC 계산 가능, Naive Bayes 분류기

---

## 가설 검정

- 귀무가설($H_0$) : 주장하려는 바의 반대 내용
- 대립가설($H_1$) : 주장하려는 바에 부합하는 내용

### 검정 통계량

- Z 통계량
  
  모분산을 알 때 모집단 평균 비교를 위해 사용
  
  표준정규분포 N(0,1) 사용
  
- t 통계량
  
  모분산을 모르는 경우 모집단 평균 비교를 위해 사용
  
  t-분포(df=n-1) 사용
  
- F 통계량
 
  두 집단 이상의 분산 비교 또는 분산분석(ANOVA)을 위해 사용
  
  F-분포(df1, df2) 사용

### p-value

귀무가설이 참이라는 전제 하에 현재 관측된 데이터보다 극단적인 결과가 나올 확률

[계산 방법]

검정통계량 계산한 뒤 귀무가설 하에 해당 통계량이 나올 확률을 누적해서 계산

유의수준 > p값 $\rightarrow$ $H_0$ do reject!

[주의할점]

1. p값 > 유의수준 = 귀무가설 채택 ?
   > (X) p값이 높으면 대립가설을 채택하지 못하는 것 뿐!

2. p값 0.03 = 귀무가설 틀릴 확률이 3%?
   > (X) $P(데이터|H_0가\ 참) \neq P(H_0가\ 틀림|데이터)$

### A/B test

[정의]

통제군과 처치군의 반응 차이를 비교해 처치 효과를 추정하는 실험 설계 방법

---

## MLE와 MAP

### MLE

[정의]

관측된 데이터가 가장 가능성이 높게 발생하도록하는 모델 파라미터 $\theta$을 찾는 방법

* 우도 : 어떤 파라미터가 주어졌을 때 데이터를 얼마나 설명하는지로 해석 가능

[기본 형태]

$\hat\theta_{MLE} = argmax_{\theta}p(X|\theta) = argmax_{\theta}\prod_{i=1}^{n}p(x_i|\theta)$

[로그 변환]

미분과 최적화를 더 쉽게 하기 위해 로그를 취함(확률 값이 0~1 사이라서 데이터가 커질수록 기본 형태 값이 작아질 수 있음)

$\hat\theta_{MLE} = argmax_{\theta}\displaystyle\sum_{i=1}^nlogp(x_i|\theta)$

**사전정보를 사용하지 않고 오직 관측된 데이터에 기반하여 계산됨**

### MAP

[정의]

관측된 데이터와 **사전 정보를 함께 고려**하여 가장 가능성이 높은 모델 파라미터를 $\theta$을 찾는 방법

[기본 형태]

$\theta_{MAP} = argmax_{\theta}p(\theta|X) = argmax_{\theta}p(X|\theta)p(\theta)$ $\leftarrow$ 정규화 상수($p(X)$) 없음 : $\theta$ 미분 과정에서 탈락됨

* $p(\theta|X) = \frac{p(X|\theta)p(\theta)}{p(X)}$

딥러닝에서의 목표 : $\hat\theta = argmin_{\theta} Loss(\theta)$

> 파라미터를 찾는 손실함수를 정의할 때 사용됨

> MAP&MLE 최대화를 하는 파라미터, Loss는 최소화 대상이므로 **마이너스 부호** 취해 사용

---

## 회귀분석 기초

### 단순회귀분석(SLR)

$$y=\beta_0+\beta_1x=u$$

독립변수가 종속변수에 대해 선형적인 영향을 미친다는 가정하에 세워진 모델

* OLS 회귀 모형

  $\hat Y_i=\hat\beta_0+\hat\beta_1X_i$

  오차의 제곱합을 최소로 만드는 것을 기준으로 계수를 결정하는 모형

[OLS가 BLUE이기 위한 조건]

  * BLUE?
  
    B : 추정량들 중 분산이 가장 작음
  
    L : 데이터 y의 선형결합으로 표현
  
    U : 추정량의 기대값이 모수의 참값과 같음
  
    E : 모수를 추정하는 식

1. 선형성

  'SLR 모델과 같이 모집단에서도 회귀계수에 대한 선형성을 나타낼 것이다'라는 가정

  독립변수와 종속변수 간의 선형성을 의미하는 것이 아닌 **핵심 파라미터에 대한 선형성** 의미

2. 랜덤 추출

   관측치가 서로 독립적이고 동일한 분포에서 왔다는 가정

   중심극한정리를 사용하기 위한 조건

3. 표본변수 내 설명변수 값 다름

   X 값이 동일하면 기울기 추정이 불가능하기 때문

4. 평균 독립

   $E[u_i|x_i] = 0$

   오차의 평균 값이 어떤 X에서든 0이라는 가정

5. 동분산

   $Var(u_i|x_i) = \sigma^2$

   오차항의 분산이 시그마 제곱(상수)으로 소정되어 X에 의존하지 않는다는 가정

   잔차 최소화하는 값 도출을 위해 각 X에서의 분산이 동일하다는 가정이 필요

   동분산 가정이 위배되어도 불편 추정량 구하기 가능 but BLUE는 아님!

---

## 모델 성능 평가 지표

### MSE

$$MSE = \frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\hat y_i)^2$$

오차를 제곱하여 평균 낸 값

제곱을 하여 더 큰 오차에 더 큰 패널티 부여

RMSE : MSE에 루트 씌운 값, 기존 y와 단위가 같아져 직관적 해석 가능

### MAE

$$MAE = \frac{1}{n}\displaystyle\sum_{i=1}^n|y_i-\hat y_i|^2$$

오차에 절댓값을 평균낸 값

이상치를 덜 강조하고 싶은 경우에 사용됨

### $R^2$(회귀)

$$R^2 = 1 - \frac{\sum(y_i-\hat y_i)^2}{\sum(y_i-\bar y)^2}$$

전체 종속변수의 변동(SST)중에서 회귀모형이 설명한 비율(설명된 변동 / 전체 변동)을 의미

- $R^2 = \frac{SSA}{SST} = 1- \frac{SSR}{SST}$

  * SST : 전체 변동 $\sum(y_i-\bar y)^2$

  * SSA : 모델이 설명한 변동 $\sum(\hat y_i - \bar y)^2$
 
  * SSR : 모델이 설명하지 못한 오차 $\sum(y_i - \hat y_i)^2$
