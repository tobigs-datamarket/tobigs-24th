{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Transformer Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "vzEqPC5PATwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention ê³„ì‚°ì‹ì„ ìƒê°í•˜ë©° ë¹ˆì¹¸ì„ ì±„ì›Œë´…ì‹œë‹¤!!"
      ],
      "metadata": {
        "id": "2q4Y7T3NCStq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# A. Scaled Dot-Product Attention\n",
        "# -------------------------\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Q,K,V: (batch, heads, seq_len, d_k)\n",
        "        mask:  (batch, 1 or heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,H,L,L)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, V)  # (B,H,L,d_k)\n",
        "        return output, attn\n"
      ],
      "metadata": {
        "id": "wLxpDVbargxb",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¬¸ì œ 1) ì•„ë˜ ì½”ë“œë¥¼ ì‚´í´ë³´ê³ , ë‹¨ìˆœ Attention ëŒ€ì‹  Multi-Head Attentionì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "\n",
        "Multi-Head Attentionì€ ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ ë‹¤ì–‘í•œ ì‹œê°ì—ì„œ íŒŒì•…í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì¸ë‹¤.\n",
        "\n",
        "ë¬¸ì œ 2) Positional Encodingì˜ ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "\n",
        "Positional Encodingì€ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ëŠ” Transformerì— ìˆœì„œ ì •ë³´ë¥¼ ì£¼ì…í•˜ì—¬ ë¬¸ë§¥ ì´í•´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "hXJz80qHmRAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# B. Multi-Head Attention\n",
        "# -------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attn = ScaledDotProductAttention(dropout=dropout)\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        B, L, D = x.shape\n",
        "        x = x.view(B, L, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def _combine_heads(self, x):\n",
        "        B, H, L, d_k = x.shape\n",
        "        x = x.transpose(1, 2).contiguous().view(B, L, H * d_k)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, kv=None, mask=None):\n",
        "        residual = x\n",
        "        if kv is None:\n",
        "            kv = x\n",
        "        Q = self._split_heads(self.W_q(x))\n",
        "        K = self._split_heads(self.W_k(kv))\n",
        "        V = self._split_heads(self.W_v(kv))\n",
        "        ctx, _ = self.attn(Q, K, V, mask=mask)          # (B,H,L_q,d_k)\n",
        "        out = self._combine_heads(ctx)                  # (B,L_q,D)\n",
        "        out = self.dropout(self.W_o(out))\n",
        "        return self.layernorm(out + residual)\n",
        "\n",
        "# -------------------------\n",
        "# C. Positional Encoding (sin/cos)\n",
        "# -------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1,L,D)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.dtype)\n",
        "        return self.dropout(x)\n",
        "\n",
        "# -------------------------\n",
        "# D. Position-wise FFN\n",
        "# -------------------------\n",
        "class PositionwiseFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.net(x)\n",
        "        return self.norm(x + residual)"
      ],
      "metadata": {
        "id": "VFSHAx-rgKwo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformerì˜ ì¸ì½”ë”ì™€ ë””ì½”ë” ë ˆì´ì–´ êµ¬ì¡°ë¥¼ ìƒê°í•˜ë©° ë¹ˆì¹¸ì„ ì±„ì›Œë´…ì‹œë‹¤!!"
      ],
      "metadata": {
        "id": "b97cQEvcm76n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# E. Encoder\n",
        "# -------------------------\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x = self.self_attn(x, kv=None, mask=src_mask)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.posenc = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        x = self.embed(src) * math.sqrt(d_model := self.embed.embedding_dim)\n",
        "        x = self.posenc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=src_mask)\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# F. Decoder\n",
        "# -------------------------\n",
        "def generate_subsequent_mask(sz: int):\n",
        "    mask = torch.tril(torch.ones(sz, sz)).bool()\n",
        "    return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.self_attn(x, kv=None, mask=tgt_mask)\n",
        "        x = self.cross_attn(x, kv=enc_out, mask=memory_mask)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.posenc = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.embed(tgt) * math.sqrt(d_model := self.embed.embedding_dim)\n",
        "        x = self.posenc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# -------------------------\n",
        "# G. ì „ì²´ Transformer + ë§ˆìŠ¤í¬\n",
        "# -------------------------\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=256, N=4, heads=4, d_ff=1024, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, N, heads, d_ff, dropout, max_len)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != PAD).unsqueeze(1).unsqueeze(1)  # (B,1,1,Ls)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        B, L = tgt.shape\n",
        "        pad = (tgt != PAD).unsqueeze(1).unsqueeze(1)   # (B,1,1,Lt)\n",
        "        causal = generate_subsequent_mask(L).to(tgt.device)  # (1,1,Lt,Lt)\n",
        "        return pad & causal\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        memory = self.encoder(src, src_mask=src_mask)\n",
        "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
        "        logits = self.generator(out)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "V6nhDAo0gXGp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Self-Supervised Learning"
      ],
      "metadata": {
        "id": "LvV1GzlY-927"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë¬¸ì œ 1) Autoencoding Language Model\n",
        "ì•„ë˜ ì„¸ ë¬¸ì¥ì—ì„œ BERTê°€ [MASK] ìœ„ì¹˜ì— ëŒ€í•´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ í† í°ì´ ë¬¸ë§¥ìƒ ì ì ˆí•œì§€ í‰ê°€í•˜ì„¸ìš”.\n",
        "\n",
        "ì ì ˆí•˜ë‹¤ë©´, ì™œ í•´ë‹¹ í† í°ì´ ìì—°ìŠ¤ëŸ½ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ”ì§€ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.\n",
        "\n",
        "ì ì ˆí•˜ì§€ ì•Šë‹¤ë©´, ê·¸ ì´ìœ ê°€ ë¬¸ë§¥ ì´í•´ ë¶€ì¡± ë•Œë¬¸ì¸ì§€, ì•„ë‹ˆë©´ í›ˆë ¨ ë°ì´í„° ë¶„í¬(ìì£¼ ë“±ì¥í•˜ëŠ” í‘œí˜„) ë•Œë¬¸ì¸ì§€ ë¶„ì„í•´ ë³´ì„¸ìš”.\n",
        "\n",
        "ì ì ˆí•˜ë‹¤. 1ë²ˆ ë‹µì€ ë¬¸ë²•ì , ì˜ë¯¸ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³ , 2ë²ˆ ë‹µë„ ì˜ë¯¸ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³ , 3ë²ˆ ë‹µë„ ì „ì¹˜ì‚¬ì™€ì˜ í˜¸ì‘ì´ ì´ë£¨ì–´ì§€ê³ , ìƒí™©ì— ë§ê²Œ ì“°ì˜€ë‹¤."
      ],
      "metadata": {
        "id": "iZoqOG5j-GKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "mlm_name = \"distilbert-base-uncased\"  # ê²½ëŸ‰ BERT\n",
        "tok = AutoTokenizer.from_pretrained(mlm_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(mlm_name)\n",
        "model.eval()\n",
        "\n",
        "def topk_mask_fill(text, k=5):\n",
        "    inputs = tok(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    mask_idx = (inputs.input_ids[0] == tok.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
        "    probs = torch.softmax(logits[0, mask_idx], dim=-1)\n",
        "    topk_ids = torch.topk(probs, k=k).indices.tolist()\n",
        "    return [(tok.decode([i]), float(probs[i])) for i in topk_ids]\n",
        "\n",
        "sentences = [\n",
        "    \"I'm wondering if I should eat [MASK] for lunch today.\",\n",
        "    \"I decided to go to the [MASK] with my friends this weekend.\",\n",
        "    \"It started to rain and I remembered I left my umbrella at [MASK].\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(\"\\nInput:\", s)\n",
        "    preds = topk_mask_fill(s, k=5) # top-k ììœ ë¡­ê²Œ ìˆ˜ì • ê°€ëŠ¥\n",
        "    for t,p in preds:\n",
        "        print(f\"  - {t:15s}  p={p:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6T28myl7-ug",
        "outputId": "a69eee5a-cdc5-4309-e161-5aa23e93af3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: I'm wondering if I should eat [MASK] for lunch today.\n",
            "  - something        p=0.0690\n",
            "  - here             p=0.0688\n",
            "  - breakfast        p=0.0414\n",
            "  - dinner           p=0.0405\n",
            "  - pizza            p=0.0374\n",
            "\n",
            "Input: I decided to go to the [MASK] with my friends this weekend.\n",
            "  - beach            p=0.1086\n",
            "  - movies           p=0.0727\n",
            "  - gym              p=0.0478\n",
            "  - mall             p=0.0325\n",
            "  - zoo              p=0.0305\n",
            "\n",
            "Input: It started to rain and I remembered I left my umbrella at [MASK].\n",
            "  - home             p=0.0831\n",
            "  - night            p=0.0602\n",
            "  - school           p=0.0366\n",
            "  - dawn             p=0.0308\n",
            "  - lunch            p=0.0271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Prompt Engineering"
      ],
      "metadata": {
        "id": "Crjl0OMiOMLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•„ë˜ëŠ” ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ Baseline Promptì™€ Engineered Promptë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ëª¨ë¸ ë‹µë³€ì´ë‹¤.\n",
        "ë‘ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³ , ì™œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§(prompt engineering)ì´ ì¤‘ìš”í•œì§€ ì„œìˆ í•˜ì‹œì˜¤.\n",
        "\n",
        "Baseline PromptëŠ”â€œConvert â€¦ formatâ€ ì •ë„ë¡œë§Œ ìš”ì²­í•˜ì˜€ê³ , Engineered PromptëŠ”â€œYou are a date parser.â€ë¡œ í–‰ë™ ë§¥ë½ì„ ê³ ì •í•˜ì˜€ë‹¤.í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì„¤ê³„í•˜ë©´ ì •í™•ë„Â·ì¼ê´€ì„±Â·íŒŒì‹±ê°€ëŠ¥ì„±ì´ í¬ê²Œ ê°œì„ ë˜ê¸° ë•Œë¬¸ì— í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ ì¤‘ìš”í•˜ë‹¤.\n",
        "\n",
        "\n",
        "ë˜í•œ, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "\n",
        "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ ì–¸ì–´ ëª¨ë¸ì— ì›í•˜ëŠ” ì¶œë ¥ì´ ë‚˜ì˜¤ë„ë¡ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„Â·ìµœì í™”í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ê°™ì€ ëª¨ë¸ì´ë¼ë„ í”„ë¡¬í”„íŠ¸ë¥¼ ì–´ë–»ê²Œ ì£¼ëŠëƒì— ë”°ë¼ ê²°ê³¼ í’ˆì§ˆì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "nW_Q66vormft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# -----------------------------\n",
        "# 1) ëª¨ë¸ ë¡œë“œ\n",
        "# -----------------------------\n",
        "model_id = \"google/flan-t5-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "def generate(prompt, max_new_tokens=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) ì˜ˆì œ í”„ë¡¬í”„íŠ¸ë“¤ (Baseline vs Engineered)\n",
        "# -----------------------------\n",
        "prompts = {\n",
        "    \"convert date\": {\n",
        "        \"baseline\": '''Convert March 5th, 2024 to YYYY-MM-DD format ''',\n",
        "\n",
        "        \"engineered\": '''You are a date parser.\n",
        "Task: Convert the input into exactly YYYY-MM-DD format (4-digit year, 2-digit month, 2-digit day).\n",
        "Rules:\n",
        "- Output ONLY the date in that format.\n",
        "- No extra text or explanation.\n",
        "Input: \"March 5th, 2024\"\n",
        "Output:'''\n",
        "    }\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) ì‹¤í–‰ ë° ë¹„êµ ì¶œë ¥\n",
        "# -----------------------------\n",
        "for task, variants in prompts.items():\n",
        "    print(\"=\"*80)\n",
        "    print(f\"ğŸ“ Task: {task}\")\n",
        "\n",
        "    for kind, prompt in variants.items():\n",
        "        output = generate(prompt)\n",
        "        print(f\"\\n--- {kind.upper()} Prompt ---\")\n",
        "        print(prompt)\n",
        "        print(f\"\\nğŸ‘‰ Model Output:\\n{output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1kmHcdHc2p5",
        "outputId": "6f401120-e84a-46eb-ab20-5dcdb32b2cb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ“ Task: convert date\n",
            "\n",
            "--- BASELINE Prompt ---\n",
            "Convert March 5th, 2024 to YYYY-MM-DD format \n",
            "\n",
            "ğŸ‘‰ Model Output:\n",
            "5th, 2024\n",
            "\n",
            "\n",
            "--- ENGINEERED Prompt ---\n",
            "You are a date parser.\n",
            "Task: Convert the input into exactly YYYY-MM-DD format (4-digit year, 2-digit month, 2-digit day).\n",
            "Rules:\n",
            "- Output ONLY the date in that format.\n",
            "- No extra text or explanation.\n",
            "Input: \"March 5th, 2024\"\n",
            "Output:\n",
            "\n",
            "ğŸ‘‰ Model Output:\n",
            "\"5/05/2024\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.RAG\n"
      ],
      "metadata": {
        "id": "1yPb-Rz8Ia49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_community sentence-transformers faiss-cpu transformers accelerate langchain-core langchain-upstage bitsandbytes"
      ],
      "metadata": {
        "id": "Oy1CGdZYJ-yK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë¬¸ì œ1) ì•„ë˜ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ ì„œìˆ í•´ì£¼ì„¸ìš”\n",
        "\n",
        "#### ë¬¸ì œ2) sample.md ë¥¼ ì—…ë¡œë“œí•´ì„œ ì•„ë˜ ìƒ˜í”Œ ì§ˆë¬¸ë“¤ì„ ì…ë ¥í•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ë³´ì„¸ìš”.\n",
        "\n",
        "1. \"ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ì—ì„œ íŠœë§ í…ŒìŠ¤íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
        "2. \"ë”¥ëŸ¬ë‹ í˜ëª…ì€ ì–¸ì œ ì‹œì‘ë˜ì—ˆë‚˜ìš”?\"\n",
        "3. \"ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”\"\n"
      ],
      "metadata": {
        "id": "AgSeT4dsFqqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, tempfile\n",
        "from google.colab import files\n",
        "from langchain_upstage import ChatUpstage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# íŒŒì¼ ì—…ë¡œë“œ í•¨ìˆ˜\n",
        "def upload_file():\n",
        "    print(\"ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
        "    uploaded = files.upload()\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "    print(f\"ì—…ë¡œë“œ ì™„ë£Œ: {file_path}\")\n",
        "    return file_path\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Solar-Pro2 LLM ë¡œë“œ\n",
        "# -----------------------------\n",
        "chat = ChatUpstage(\n",
        "    api_key=\"up_*************************jMaZ\",\n",
        "    model=\"solar-pro2\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Colab RAG ì‹œìŠ¤í…œ ì •ì˜\n",
        "# -----------------------------\n",
        "def colab_rag_system(file_path):\n",
        "    # 1. ë¬¸ì„œ ë¡œë“œ\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    print(f\"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(documents)} ê°œì˜ ë¬¸ì„œ\")\n",
        "\n",
        "    # 2. ë¬¸ì„œ ë¶„í• \n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(texts)} ê°œì˜ ì²­í¬\")\n",
        "\n",
        "    # 3. ì„ë² ë”© + ë²¡í„°ì €ì¥ì†Œ\n",
        "    print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    print(\"ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "    # 4. ê²€ìƒ‰ê¸° ìƒì„±\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "    # 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "    prompt_template = \"\"\"\n",
        "    ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.\n",
        "    ë§Œì•½ ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ \"ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    ì§ˆë¬¸: {question}\n",
        "    ë‹µë³€:\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # 6. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
        "    print(\"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì¤‘...\")\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=chat,                    # ì—¬ê¸°ì„œ Solar-Pro2 ì‚¬ìš©\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )\n",
        "\n",
        "    print(\"RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "\n",
        "    # 7. ëŒ€í™”í˜• ì§ˆì˜\n",
        "    while True:\n",
        "        query = input(\"\\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥): \")\n",
        "        if query.lower() == 'q':\n",
        "            break\n",
        "\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "        print(\"\\në‹µë³€:\", result[\"result\"])\n",
        "\n",
        "\n",
        "    # 8. ë²¡í„° ì €ì¥ì†Œ ì €ì¥\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        index_path = os.path.join(temp_dir, \"faiss_index\")\n",
        "        vectorstore.save_local(index_path)\n",
        "        print(f\"\\nì¸ë±ìŠ¤ë¥¼ '{index_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = upload_file()\n",
        "    colab_rag_system(file_path)"
      ],
      "metadata": {
        "id": "0QE4D2pvFoSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "6e255fc7-7b22-4bcc-897f-f50c1f05b1c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7bc1c74c-ecc6-4c32-9bda-2c1d9352d080\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7bc1c74c-ecc6-4c32-9bda-2c1d9352d080\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-455837758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mcolab_rag_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-455837758.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ì—…ë¡œë“œ ì™„ë£Œ: {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}