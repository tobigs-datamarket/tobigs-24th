{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Transformer Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "vzEqPC5PATwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention ê³„ì‚°ì‹ì„ ìƒê°í•˜ë©° ë¹ˆì¹¸ì„ ì±„ì›Œë´…ì‹œë‹¤!!"
      ],
      "metadata": {
        "id": "2q4Y7T3NCStq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# A. Scaled Dot-Product Attention\n",
        "# -------------------------\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Q,K,V: (batch, heads, seq_len, d_k)\n",
        "        mask:  (batch, 1 or heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,H,L,L)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, V)  # (B,H,L,d_k)\n",
        "        return output, attn\n"
      ],
      "metadata": {
        "id": "wLxpDVbargxb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¬¸ì œ 1) ì•„ë˜ ì½”ë“œë¥¼ ì‚´í´ë³´ê³ , ë‹¨ìˆœ Attention ëŒ€ì‹  Multi-Head Attentionì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "\n",
        "- Multi-head attentionì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ í•˜ë‚˜ì˜ ë¬¸ì¥ í‘œí˜„ì— ëŒ€í•´ì„œ ë‹¤ì–‘í•˜ê²Œ(ê·¼ì ‘ ë¬¸ë§¥, ì¥ê±°ë¦¬ ë¬¸ë§¥ ë“±) í•™ìŠµí•˜ê²Œ ë˜ì–´ ì—¬ëŸ¬ ê´€ê³„ë¥¼ í•œë²ˆì— í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "ë¬¸ì œ 2) Positional Encodingì˜ ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "\n",
        "- Attention ì—°ì‚° ì‹œ ìˆœì„œì— ìƒê´€ì—†ì´ ì—°ì‚°ì´ ë˜ëŠ”ë°, ìˆœì„œë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ positional encodingì„ ë”°ë¡œ ë”í•´ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ë³´ì¡´í•œë‹¤.\n"
      ],
      "metadata": {
        "id": "hXJz80qHmRAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# B. Multi-Head Attention\n",
        "# -------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attn = ScaledDotProductAttention(dropout=dropout)\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        B, L, D = x.shape\n",
        "        x = x.view(B, L, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def _combine_heads(self, x):\n",
        "        B, H, L, d_k = x.shape\n",
        "        x = x.transpose(1, 2).contiguous().view(B, L, H * d_k)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, kv=None, mask=None):\n",
        "        residual = x\n",
        "        if kv is None:\n",
        "            kv = x\n",
        "        Q = self._split_heads(self.W_q(x))\n",
        "        K = self._split_heads(self.W_k(kv))\n",
        "        V = self._split_heads(self.W_v(kv))\n",
        "        ctx, _ = self.attn(Q, K, V, mask=mask)          # (B,H,L_q,d_k)\n",
        "        out = self._combine_heads(ctx)                  # (B,L_q,D)\n",
        "        out = self.dropout(self.W_o(out))\n",
        "        return self.layernorm(out + residual)\n",
        "\n",
        "# -------------------------\n",
        "# C. Positional Encoding (sin/cos)\n",
        "# -------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1,L,D)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.dtype)\n",
        "        return self.dropout(x)\n",
        "\n",
        "# -------------------------\n",
        "# D. Position-wise FFN\n",
        "# -------------------------\n",
        "class PositionwiseFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.net(x)\n",
        "        return self.norm(x + residual)"
      ],
      "metadata": {
        "id": "VFSHAx-rgKwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformerì˜ ì¸ì½”ë”ì™€ ë””ì½”ë” ë ˆì´ì–´ êµ¬ì¡°ë¥¼ ìƒê°í•˜ë©° ë¹ˆì¹¸ì„ ì±„ì›Œë´…ì‹œë‹¤!!"
      ],
      "metadata": {
        "id": "b97cQEvcm76n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# E. Encoder\n",
        "# -------------------------\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x = self.self_attn(x, kv=None, mask=src_mask)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.posenc = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        x = self.embed(src) * math.sqrt(d_model := self.embed.embedding_dim)\n",
        "        x = self.posenc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=src_mask)\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# F. Decoder\n",
        "# -------------------------\n",
        "def generate_subsequent_mask(sz: int):\n",
        "    mask = torch.tril(torch.ones(sz, sz)).bool()\n",
        "    return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.self_attn(x, kv=None, mask=tgt_mask)\n",
        "        x = self.cross_attn(x, kv=enc_out, mask=memory_mask)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.posenc = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.embed(tgt) * math.sqrt(d_model := self.embed.embedding_dim)\n",
        "        x = self.posenc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# -------------------------\n",
        "# G. ì „ì²´ Transformer + ë§ˆìŠ¤í¬\n",
        "# -------------------------\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=256, N=4, heads=4, d_ff=1024, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, N, heads, d_ff, dropout, max_len)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != PAD).unsqueeze(1).unsqueeze(1)  # (B,1,1,Ls)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        B, L = tgt.shape\n",
        "        pad = (tgt != PAD).unsqueeze(1).unsqueeze(1)   # (B,1,1,Lt)\n",
        "        causal = generate_subsequent_mask(L).to(tgt.device)  # (1,1,Lt,Lt)\n",
        "        return pad & causal\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        memory = self.encoder(src, src_mask=src_mask)\n",
        "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
        "        logits = self.generator(out)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "V6nhDAo0gXGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Self-Supervised Learning"
      ],
      "metadata": {
        "id": "LvV1GzlY-927"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë¬¸ì œ 1) Autoencoding Language Model\n",
        "ì•„ë˜ ì„¸ ë¬¸ì¥ì—ì„œ BERTê°€ [MASK] ìœ„ì¹˜ì— ëŒ€í•´ ì˜ˆì¸¡í•œ 1ìˆœìœ„ í† í°ì´ ë¬¸ë§¥ìƒ ì ì ˆí•œì§€ í‰ê°€í•˜ì„¸ìš”.\n",
        "\n",
        "ì ì ˆí•˜ë‹¤ë©´, ì™œ í•´ë‹¹ í† í°ì´ ìì—°ìŠ¤ëŸ½ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ”ì§€ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.\n",
        "\n",
        "ì ì ˆí•˜ì§€ ì•Šë‹¤ë©´, ê·¸ ì´ìœ ê°€ ë¬¸ë§¥ ì´í•´ ë¶€ì¡± ë•Œë¬¸ì¸ì§€, ì•„ë‹ˆë©´ í›ˆë ¨ ë°ì´í„° ë¶„í¬(ìì£¼ ë“±ì¥í•˜ëŠ” í‘œí˜„) ë•Œë¬¸ì¸ì§€ ë¶„ì„í•´ ë³´ì„¸ìš”.\n",
        "\n",
        "- BERTê°€ ì˜ˆì¸¡í•œ 1ìˆœìœ„ í† í°ë“¤ì„ ë³´ë©´ ë¬¸ë§¥ ìƒí™©ê³¼ ê³ ë ¤í–ˆì„ ë•Œ ì ì ˆí•œ í† í°ì´ë‹¤."
      ],
      "metadata": {
        "id": "iZoqOG5j-GKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "mlm_name = \"distilbert-base-uncased\"  # ê²½ëŸ‰ BERT\n",
        "tok = AutoTokenizer.from_pretrained(mlm_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(mlm_name)\n",
        "model.eval()\n",
        "\n",
        "def topk_mask_fill(text, k=5):\n",
        "    inputs = tok(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    mask_idx = (inputs.input_ids[0] == tok.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
        "    probs = torch.softmax(logits[0, mask_idx], dim=-1)\n",
        "    topk_ids = torch.topk(probs, k=k).indices.tolist()\n",
        "    return [(tok.decode([i]), float(probs[i])) for i in topk_ids]\n",
        "\n",
        "sentences = [\n",
        "    \"I'm wondering if I should eat [MASK] for lunch today.\",\n",
        "    \"I decided to go to the [MASK] with my friends this weekend.\",\n",
        "    \"It started to rain and I remembered I left my umbrella at [MASK].\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(\"\\nInput:\", s)\n",
        "    preds = topk_mask_fill(s, k=5) # top-k ììœ ë¡­ê²Œ ìˆ˜ì • ê°€ëŠ¥\n",
        "    for t,p in preds:\n",
        "        print(f\"  - {t:15s}  p={p:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682,
          "referenced_widgets": [
            "6a31ad309e91430e8cb2a91f749ef907",
            "d3d23329faf946e4b9e418b2d52c5b93",
            "8ff1f48460e44cd681452de7826f028e",
            "f58c9c1f488e4fdeb35386e9c70c1c79",
            "ca24824244794ae095e3ba0afbcf6ad7",
            "bee295b5b68240c2b496c98e27f336ec",
            "43498bcfc7004c5f8f907d8f3fa36e21",
            "bccd023ffb124c568702a976d50a123f",
            "60db22feadb84951b3f8540d7b19ef29",
            "568383bd8c424bf6ad29acbeb01fcaf0",
            "adb32648086c4e818a876b17f8c52c60",
            "b7b74b482c7448ec9961438580037b02",
            "ba0f92e078b74573837e765f511739b2",
            "c8717b3c652141e5a58216cc8ed73bed",
            "ab7e9916db594d5780e6bfa07559bd6a",
            "94b5df2e648c493eab5cf3bb4e447321",
            "ef8187667ac345cd9d751111c409283e",
            "8c4e84e25e164a8893953e77b175f704",
            "debc8cf2cd724e98a782dbf1a1cfc874",
            "39ee39feb0494c3798ffe415e587c871",
            "406f0116710f4a7884c0859c1f746e5a",
            "58e80cac452b422f91656cb25f66a326",
            "e35d514684074acaaf9129eb250294b7",
            "f11378ce25824e1aa3fb9248eeb56c58",
            "9945f6b8e3014a2e9fc3805c987dc56d",
            "0d16448da33f40bc92ea0abf51755fb4",
            "6cb288deef354ac1ac229e2a08f00b97",
            "4a89664866014c37b14b1d1da3ce2261",
            "7643cc54301742b88b1743279fafa047",
            "002c543a48e0449b973cd1bb144309dd",
            "d65c612e701c47699518a1dd56a494a3",
            "12e17af0523e4793a733ebe986758759",
            "4464a1c97f4d473eaad0eb5ee9debfee",
            "f0c5461bc58742bc8d78c7a422e5d4f6",
            "abcb42db13164655991ec2d3ee627665",
            "ede9508ecdef4fdc83a2323ad300bc45",
            "80ec97f45b3f4d1f910b88ff07c7fd9a",
            "8b0ec289b65a492c95f28e762c06dbd0",
            "fc615149b6874d02b65bbc542b3fef15",
            "d8360269bc804d30bda669e11b24166c",
            "45bf93be3ba44f6ba10aa4e25da50e06",
            "dc20ab26599941589092a515867c2a79",
            "13893bf4a5964ea9936bc29c1c4bfea3",
            "a55206d2f7c24d86b248bc868cb75472",
            "30b38d0022584e6bbba148eb6ddc4032",
            "adc9790468424a99999c010bb82d14c2",
            "0745dc4a4fd247608ad0562e844bd8f3",
            "bb01ad4ac5d5425984b8bf9258f747f4",
            "cbf87159c335437bbb6cfb75f1964ace",
            "23cdeee13fff4bdf9ca669bcf0cd5af9",
            "cf2cb32c31df4fffb6bed619c00847ba",
            "0e172c5d7fce4b57b74e9257522e3130",
            "be4b8f4e977f406f8faf9c18a8976f7d",
            "570fcda3efd1403b9fcba733d3de7555",
            "3fe8eee9d77c480193e16f2f63db8cba"
          ]
        },
        "id": "z6T28myl7-ug",
        "outputId": "43ba056f-842b-469d-9613-469dadee77ab",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1757211193370,
          "user_tz": -540,
          "elapsed": 27517,
          "user": {
            "displayName": "ê¹€ì¤€ìˆ˜",
            "userId": "12842195215021127123"
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a31ad309e91430e8cb2a91f749ef907"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7b74b482c7448ec9961438580037b02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e35d514684074acaaf9129eb250294b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0c5461bc58742bc8d78c7a422e5d4f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b38d0022584e6bbba148eb6ddc4032"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: I'm wondering if I should eat [MASK] for lunch today.\n",
            "  - something        p=0.0690\n",
            "  - here             p=0.0688\n",
            "  - breakfast        p=0.0414\n",
            "  - dinner           p=0.0405\n",
            "  - pizza            p=0.0374\n",
            "\n",
            "Input: I decided to go to the [MASK] with my friends this weekend.\n",
            "  - beach            p=0.1086\n",
            "  - movies           p=0.0727\n",
            "  - gym              p=0.0478\n",
            "  - mall             p=0.0325\n",
            "  - zoo              p=0.0305\n",
            "\n",
            "Input: It started to rain and I remembered I left my umbrella at [MASK].\n",
            "  - home             p=0.0831\n",
            "  - night            p=0.0602\n",
            "  - school           p=0.0366\n",
            "  - dawn             p=0.0308\n",
            "  - lunch            p=0.0271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Prompt Engineering"
      ],
      "metadata": {
        "id": "Crjl0OMiOMLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•„ë˜ëŠ” ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ Baseline Promptì™€ Engineered Promptë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ëª¨ë¸ ë‹µë³€ì´ë‹¤.\n",
        "ë‘ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³ , ì™œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§(prompt engineering)ì´ ì¤‘ìš”í•œì§€ ì„œìˆ í•˜ì‹œì˜¤.\n",
        "\n",
        "ë˜í•œ, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "\n",
        "- prompt engineeringì€ userì—ê²Œ ì •í™•í•˜ê³  íŠ¹í™”ëœ ë‹µë³€ì„ ì¤„ ìˆ˜ ìˆê²Œ promptë¥¼ ë””ìì¸í•˜ëŠ” ê²ƒì´ë‹¤. prompt engineeringì´ ì¤‘ìš”í•œ ì´ìœ ëŠ”, ì¼ê´€ëœ ë‹µë³€ì„ ë°›ë„ë¡ ìœ ë„í•  ìˆ˜ ìˆê³ , ë‹¤ì–‘í•œ ê³¼ì •ì„ ì§€ì‹œí•  ìˆ˜ ìˆì–´ ë‹µë³€ì˜ í€„ë¦¬í‹°ê°€ ë†’ì•„ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ hallucinationì„ ë°©ì§€í•  ìˆ˜ë„ ìˆë‹¤."
      ],
      "metadata": {
        "id": "nW_Q66vormft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# -----------------------------\n",
        "# 1) ëª¨ë¸ ë¡œë“œ\n",
        "# -----------------------------\n",
        "model_id = \"google/flan-t5-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "def generate(prompt, max_new_tokens=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) ì˜ˆì œ í”„ë¡¬í”„íŠ¸ë“¤ (Baseline vs Engineered)\n",
        "# -----------------------------\n",
        "prompts = {\n",
        "    \"convert date\": {\n",
        "        \"baseline\": '''Convert March 5th, 2024 to YYYY-MM-DD format ''',\n",
        "\n",
        "        \"engineered\": '''You are a date parser.\n",
        "Task: Convert the input into exactly YYYY-MM-DD format (4-digit year, 2-digit month, 2-digit day).\n",
        "Rules:\n",
        "- Output ONLY the date in that format.\n",
        "- No extra text or explanation.\n",
        "Input: \"March 5th, 2024\"\n",
        "Output:'''\n",
        "    }\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) ì‹¤í–‰ ë° ë¹„êµ ì¶œë ¥\n",
        "# -----------------------------\n",
        "for task, variants in prompts.items():\n",
        "    print(\"=\"*80)\n",
        "    print(f\"ğŸ“ Task: {task}\")\n",
        "\n",
        "    for kind, prompt in variants.items():\n",
        "        output = generate(prompt)\n",
        "        print(f\"\\n--- {kind.upper()} Prompt ---\")\n",
        "        print(prompt)\n",
        "        print(f\"\\nğŸ‘‰ Model Output:\\n{output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652,
          "referenced_widgets": [
            "2c74d0b3e8104c3b8c23374bcdb4e298",
            "87615cbd50294548884cdb4b43a8b92a",
            "9a061c55ad51473cb75c239fa2eac498",
            "de2d74861ff74ff9ba65129ba45137cf",
            "a898537c092c405f9c5c7e04f5198de5",
            "9a62514ae3b04af09d82edc2f5153b75",
            "7e664dd656d94f899ada65072db1a3cb",
            "b95595ba7dda4ed1a080eef54862ca77",
            "9c01ef6cb35c40f99220a43fac1018b4",
            "bf2d60200ad94bb0a9dbe54b65a8d0d0",
            "a8ffce4b91b74e3d9b45a5aec560d157",
            "3edc568a5bf24103a5739a154bfe88a6",
            "e21e8026dc5d4b40a62b199200c6210e",
            "fc34e314f83748c48ca96a2ee6cd9d09",
            "157a0cdd899c427bb7ca92ddd0e1cc4a",
            "ab48de2affbf4c448a87d2ba2c231f7b",
            "fb7fba80bc0e4e3ab1a4d06d03a4836f",
            "7ba833a550594e0e91cccd91dee8e32d",
            "b419cbc08430498fb0d9deac12bfc638",
            "12bd3331c2eb4a13aa0f149b9193a03a",
            "5d0b2d546fbb431c9e6fc1b014ddb4fd",
            "dd324b2d788d424583346e97040813a9",
            "760d072135574173992868b7206bde45",
            "27b647d8c3e7417a940406fbd1f7dd34",
            "de93e7eb5dd94b06b137e336eaf2ada5",
            "2b70cb945378442daf0fa656fac7aa82",
            "c7228a6179cc422ba7a9227b6883e9c2",
            "3fa01ed11f9b4cc98dbc73e30579b61f",
            "6cdefcf00afc42cc919dcabe0047135a",
            "7d0c6339c29e47f789bf6b62b5d9a3b6",
            "d85f86641220492cbe677aa5df679bc8",
            "240608fac55a4c4b9c42dc09c311d12f",
            "16609e2dc27b450db39821e3a4edb66f",
            "8d4051a5b6fb45159b472e71af9e513f",
            "1584b3029bdc49f38b85222231263c39",
            "124ea12d4c8649f29a37743b216957bc",
            "c0e33f0ed0ce406aa3cbecb4df023551",
            "ac127c438ce94a0bb78c90b2e8998d0a",
            "5acb6799d1d540f9ae2b2ce1439085f2",
            "94832cc48a11455aa9fc069ba0230a61",
            "a2ee4188a26348839b721f6c4eccc08e",
            "978e3aa4b8554a74ad72563221951d04",
            "200c56aba986425bbfd0efd211d973f1",
            "e30bbfb90ec84820b83e70268663e1b1",
            "62b2c668e41942239fbbb7a27d9f4337",
            "c8bfc52935ea458aa795e6a459b6b1d7",
            "2fa20ea632cc4efc8125e55f43554dc8",
            "34fd05e30f7945329095362e807fa70e",
            "2b523072b4a5494cb6da9eee3b8d4a15",
            "0d3b7af7821649ae9cca7595bb53f4ec",
            "3ed4ff451c684a53ab6219c58b130c63",
            "10d523aa28374c8eb37c4a899c3ec832",
            "f5bad9bf14304d62a995d89de5287b92",
            "c95e5956f6de4650acaec656e02bdeb8",
            "ee311b597a4046328f346149cc338886",
            "750456e382fc449cb3a5066bf634ad5d",
            "375bd1c80544481c844792a79f678255",
            "cd3a663f1f7145bc9399b10931f258d1",
            "01dcc31286514239b334010ea76620bd",
            "2190dc1a94a24129b594173d213602d2",
            "25481e2ffa07402f83ad3db3eb8a7d93",
            "2e35be73f3924d2f84ad076678a10be4",
            "02c30154ba4941718d29f460e4504aff",
            "e9f6de4403e14ceb8e073b09e6fbcccb",
            "6fbf28048d0e4bd78d4366f3aef7ce10",
            "affdb9af9d574ce5af69da51f008f416",
            "57980b1a84d34e0aac38d27bb41a424d",
            "d8693ac1a11240ac83c4d4d29c54352f",
            "b7fc8e1db174432fa7ef2d08fbd6ea41",
            "b0070513d7a34a83975f9ac254e9692e",
            "716cb76d6dcd4baebe1e3a4e8f65fa1c",
            "3890653609d345bca9cb529ab50201b8",
            "10581e05a8e4404ab811c74711a6d32f",
            "09259b3e0b324f18b50453a76f4d9ab0",
            "a266043f82d84b219436e2e4e270e20e",
            "aa0cb4c680e549c09495c5d609f785e3",
            "fe0954a23fbe45da8908551beb7e6925"
          ]
        },
        "id": "n1kmHcdHc2p5",
        "outputId": "e476ae47-65f3-4e96-e524-11e04cc6df3f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1757211305143,
          "user_tz": -540,
          "elapsed": 21940,
          "user": {
            "displayName": "ê¹€ì¤€ìˆ˜",
            "userId": "12842195215021127123"
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c74d0b3e8104c3b8c23374bcdb4e298"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3edc568a5bf24103a5739a154bfe88a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "760d072135574173992868b7206bde45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d4051a5b6fb45159b472e71af9e513f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62b2c668e41942239fbbb7a27d9f4337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "750456e382fc449cb3a5066bf634ad5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57980b1a84d34e0aac38d27bb41a424d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ“ Task: convert date\n",
            "\n",
            "--- BASELINE Prompt ---\n",
            "Convert March 5th, 2024 to YYYY-MM-DD format \n",
            "\n",
            "ğŸ‘‰ Model Output:\n",
            "5th, 2024\n",
            "\n",
            "\n",
            "--- ENGINEERED Prompt ---\n",
            "You are a date parser.\n",
            "Task: Convert the input into exactly YYYY-MM-DD format (4-digit year, 2-digit month, 2-digit day).\n",
            "Rules:\n",
            "- Output ONLY the date in that format.\n",
            "- No extra text or explanation.\n",
            "Input: \"March 5th, 2024\"\n",
            "Output:\n",
            "\n",
            "ğŸ‘‰ Model Output:\n",
            "\"5/05/2024\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.RAG\n"
      ],
      "metadata": {
        "id": "1yPb-Rz8Ia49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_community sentence-transformers faiss-cpu transformers accelerate langchain-core langchain-upstage bitsandbytes"
      ],
      "metadata": {
        "id": "Oy1CGdZYJ-yK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e2cc0c-a6b8-4bca-bfed-9fb9c04e2558",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1757211483352,
          "user_tz": -540,
          "elapsed": 40005,
          "user": {
            "displayName": "ê¹€ì¤€ìˆ˜",
            "userId": "12842195215021127123"
          }
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVEaUv5a-KLV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1757211580145,
          "user_tz": -540,
          "elapsed": 8817,
          "user": {
            "displayName": "ê¹€ì¤€ìˆ˜",
            "userId": "12842195215021127123"
          }
        },
        "outputId": "204527aa-a0f1-4ca9-9551-8b581ce46f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë¬¸ì œ1) ì•„ë˜ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ ì„œìˆ í•´ì£¼ì„¸ìš”\n",
        "\n",
        "#### ë¬¸ì œ2) sample.md ë¥¼ ì—…ë¡œë“œí•´ì„œ ì•„ë˜ ìƒ˜í”Œ ì§ˆë¬¸ë“¤ì„ ì…ë ¥í•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ë³´ì„¸ìš”.\n",
        "\n",
        "1. \"ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ì—ì„œ íŠœë§ í…ŒìŠ¤íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
        "2. \"ë”¥ëŸ¬ë‹ í˜ëª…ì€ ì–¸ì œ ì‹œì‘ë˜ì—ˆë‚˜ìš”?\"\n",
        "3. \"ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”\"\n",
        "\n",
        "\n",
        "-> sentence_transformer ë²„ì „ ì˜¤ë¥˜ë¡œ ì‹¤í–‰ì´ ì•ˆë©ë‹ˆë‹¤ .. ã… "
      ],
      "metadata": {
        "id": "AgSeT4dsFqqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, tempfile\n",
        "# from google.colab import files\n",
        "# from langchain_upstage import ChatUpstage\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# from langchain_core.messages import HumanMessage\n",
        "# from langchain_community.document_loaders import TextLoader\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "# # íŒŒì¼ ì—…ë¡œë“œ í•¨ìˆ˜\n",
        "# def upload_file():\n",
        "#     print(\"ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
        "#     uploaded = files.upload()\n",
        "#     file_path = list(uploaded.keys())[0]\n",
        "#     print(f\"ì—…ë¡œë“œ ì™„ë£Œ: {file_path}\")\n",
        "#     return file_path\n",
        "\n",
        "# # -----------------------------\n",
        "# # 1) Solar-Pro2 LLM ë¡œë“œ\n",
        "# # -----------------------------\n",
        "# chat = ChatUpstage(\n",
        "#     api_key=\"up_qwEgtTW1CNtpfl7ZeIb9MUmsWHIBp\",\n",
        "#     model=\"solar-pro2\"\n",
        "# )\n",
        "\n",
        "# # -----------------------------\n",
        "# # 2) Colab RAG ì‹œìŠ¤í…œ ì •ì˜\n",
        "# # -----------------------------\n",
        "# def colab_rag_system(file_path):\n",
        "#     # 1. ë¬¸ì„œ ë¡œë“œ\n",
        "#     loader = TextLoader(file_path)\n",
        "#     documents = loader.load()\n",
        "#     print(f\"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(documents)} ê°œì˜ ë¬¸ì„œ\")\n",
        "\n",
        "#     # 2. ë¬¸ì„œ ë¶„í• \n",
        "#     text_splitter = RecursiveCharacterTextSplitter(\n",
        "#         chunk_size=1000,\n",
        "#         chunk_overlap=50,\n",
        "#         length_function=len\n",
        "#     )\n",
        "#     texts = text_splitter.split_documents(documents)\n",
        "#     print(f\"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(texts)} ê°œì˜ ì²­í¬\")\n",
        "\n",
        "#     # 3. ì„ë² ë”© + ë²¡í„°ì €ì¥ì†Œ\n",
        "#     print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
        "#     embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "#     print(\"ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...\")\n",
        "#     vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "#     # 4. ê²€ìƒ‰ê¸° ìƒì„±\n",
        "#     retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "#     # 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "#     prompt_template = \"\"\"\n",
        "#     ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.\n",
        "#     ë§Œì•½ ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ \"ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.\n",
        "\n",
        "#     {context}\n",
        "\n",
        "#     ì§ˆë¬¸: {question}\n",
        "#     ë‹µë³€:\n",
        "#     \"\"\"\n",
        "#     PROMPT = PromptTemplate(\n",
        "#         template=prompt_template,\n",
        "#         input_variables=[\"context\", \"question\"]\n",
        "#     )\n",
        "\n",
        "#     # 6. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
        "#     print(\"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì¤‘...\")\n",
        "#     qa_chain = RetrievalQA.from_chain_type(\n",
        "#         llm=chat,                    # ì—¬ê¸°ì„œ Solar-Pro2 ì‚¬ìš©\n",
        "#         chain_type=\"stuff\",\n",
        "#         retriever=retriever,\n",
        "#         return_source_documents=True,\n",
        "#         chain_type_kwargs={\"prompt\": PROMPT}\n",
        "#     )\n",
        "\n",
        "#     print(\"RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "\n",
        "#     # 7. ëŒ€í™”í˜• ì§ˆì˜\n",
        "#     while True:\n",
        "#         query = input(\"\\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥): \")\n",
        "#         if query.lower() == 'q':\n",
        "#             break\n",
        "\n",
        "#         result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "#         print(\"\\në‹µë³€:\", result[\"result\"])\n",
        "\n",
        "\n",
        "#     # 8. ë²¡í„° ì €ì¥ì†Œ ì €ì¥\n",
        "#     with tempfile.TemporaryDirectory() as temp_dir:\n",
        "#         index_path = os.path.join(temp_dir, \"faiss_index\")\n",
        "#         vectorstore.save_local(index_path)\n",
        "#         print(f\"\\nì¸ë±ìŠ¤ë¥¼ '{index_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     file_path = upload_file()\n",
        "#     colab_rag_system(file_path)\n",
        "\n",
        "## Error Message\n",
        "\n",
        "'''\n",
        "ImportError                               Traceback (most recent call last)\n",
        "/usr/local/lib/python3.12/dist-packages/langchain_community/embeddings/huggingface.py in __init__(self, **kwargs)\n",
        "     83         try:\n",
        "---> 84             import sentence_transformers\n",
        "     85\n",
        "\n",
        "14 frames\n",
        "ImportError: cannot import name 'add_model_info_to_auto_map' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)\n",
        "\n",
        "The above exception was the direct cause of the following exception:\n",
        "\n",
        "ImportError                               Traceback (most recent call last)\n",
        "/usr/local/lib/python3.12/dist-packages/langchain_community/embeddings/huggingface.py in __init__(self, **kwargs)\n",
        "     85\n",
        "     86         except ImportError as exc:\n",
        "---> 87             raise ImportError(\n",
        "     88                 \"Could not import sentence_transformers python package. \"\n",
        "     89                 \"Please install it with `pip install sentence-transformers`.\"\n",
        "\n",
        "ImportError: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "NOTE: If your import is failing due to a missing package, you can\n",
        "manually install dependencies using either !pip or !apt.\n",
        "\n",
        "To view examples of installing some common dependencies, click the\n",
        "\"Open Examples\" button below.\n",
        "'''"
      ],
      "metadata": {
        "id": "0QE4D2pvFoSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "66debbf2-ea3f-4604-e0d2-7ab02aa3a42a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1757215940909,
          "user_tz": -540,
          "elapsed": 51,
          "user": {
            "displayName": "ê¹€ì¤€ìˆ˜",
            "userId": "12842195215021127123"
          }
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nImportError                               Traceback (most recent call last)\\n/usr/local/lib/python3.12/dist-packages/langchain_community/embeddings/huggingface.py in __init__(self, **kwargs)\\n     83         try:\\n---> 84             import sentence_transformers\\n     85 \\n\\n14 frames\\nImportError: cannot import name \\'add_model_info_to_auto_map\\' from \\'transformers.utils\\' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)\\n\\nThe above exception was the direct cause of the following exception:\\n\\nImportError                               Traceback (most recent call last)\\n/usr/local/lib/python3.12/dist-packages/langchain_community/embeddings/huggingface.py in __init__(self, **kwargs)\\n     85 \\n     86         except ImportError as exc:\\n---> 87             raise ImportError(\\n     88                 \"Could not import sentence_transformers python package. \"\\n     89                 \"Please install it with `pip install sentence-transformers`.\"\\n\\nImportError: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.\\n\\n---------------------------------------------------------------------------\\nNOTE: If your import is failing due to a missing package, you can\\nmanually install dependencies using either !pip or !apt.\\n\\nTo view examples of installing some common dependencies, click the\\n\"Open Examples\" button below.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}